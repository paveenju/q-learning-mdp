# Training Q-learning agent in an MDP environment
This work is based on [this example]() from [mathworks.com](https://www.mathworks.com). Please enjoy attached python notebook in this repository to understand how to train Q-learning agent in a generic Markov Decision Process (MDP) environment.

To have better understanding in this work, I introduce sections as follows:
- [Reinforcement learning](#reinforcement-learning)
- [Problem modeling with MDP](#problem-modeling-with-mdp)
- [Q-learning](#q-learning)
- [Parameters](#parameters)
- [Results](#results)

## Reinforcement learning
The reinforcement learning approach [Sutton et al. 2018](http://incompleteideas.net/book/RLbook2020.pdf) selects actions that maximize expected rewards generated by decisions. The agent learns how to choose actions through trial-and-error interactions with a dynamic environment to observe the signals or rewards returned from previous states. The agent may take a long sequence of actions to have a delayed rewards, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement.

## Problem modeling with MDP
Assume that the environment of the problem is fully observable. Precisely, we can get all informations concerning all states (from begin to termianl states) and all state-to-state transitions. Therefore, we can formulate a generic MDP model. An example MDP can be represented as the following graph:


From this example, at each state there is a decision to go up or down. The agent begins from state 1. The state 7 and 8 are terminal states. The agent receives a reward equal to the value on each transition in the graph. The training goal is to collect the maximum cumulative reward.

In this work, we apply the Q-learning agent to solve this MDP environment.

## Q-learning

## Parameters

## Results
